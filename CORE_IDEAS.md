# Core Ideas of the TM5-900 Indoor Vlogger System

**Project:** Autonomous Face-Tracking Vlogger Robot
**Platform:** Techman Robot TM5-900 (6-DOF Collaborative Robot)
**Team:** Team 11
**Date:** December 2025

---

## Overview

This document explains the fundamental concepts and design philosophy behind the TM5-900 Indoor Vlogger System - an intelligent robotic camera operator that autonomously tracks and frames human subjects for video recording.

---

## Core Ideas

### 1. **Autonomous Camera Operator Concept**

The system transforms a TM5-900 robotic arm into an intelligent camera operator that autonomously tracks and frames a human subject, eliminating the need for:
- A dedicated camera operator
- Manual framing adjustments during recording
- Multiple takes to get proper positioning

**Philosophy:** Treat the robot as a smart cinematographer, not just a mechanical arm. The robot "understands" good framing and actively maintains optimal shot composition.

---

### 2. **Real-Time Face Tracking with Computer Vision**

Uses **MediaPipe Face Mesh** for high-performance face detection:
- Detects 468 facial landmarks in real-time at 30 FPS
- Calculates face bounding box, center position, and size
- Achieves sub-100ms detection latency using CPU-based inference
- Switches to USB webcam for 100Ã— performance improvement over built-in Techman camera (0.3 FPS â†’ 30 FPS)

**Key Technology:** MediaPipe's BlazeFace detector provides efficient, accurate face detection without requiring GPU hardware, making the system accessible and cost-effective.

---

### 3. **Intelligent Multi-Axis Tracking Control**

Implements a sophisticated control algorithm that maps image space to robot space:
- **Horizontal tracking**: Image X â†’ Robot Y (left/right movement)
- **Vertical tracking**: Image Y â†’ Robot Z (up/down movement)
- **Distance control**: Face size â†’ Robot X (forward/backward depth)
- Uses **calibrated mm-per-pixel conversion** based on camera FOV (55Â°) for accurate positioning

**Mathematical Foundation:**
```
visible_width_mm = 2 Ã— distance Ã— tan(FOV/2)
mm_per_pixel = visible_width_mm / image_width
robot_movement_mm = pixel_offset Ã— mm_per_pixel
```

This coordinate transformation ensures the robot moves the correct physical distance to center the subject in frame.

---

### 4. **Automatic Distance Framing**

Maintains optimal shot composition by:
- Targeting face size of ~120 pixels (Â±20px tolerance)
- Automatically moving closer when face is too small (person too far)
- Backing away when face is too large (person too close)
- Creates consistent, professional-looking framing without manual adjustment

**Cinematography Principle:** Maintains "medium shot" framing (head and shoulders visible) by keeping face size constant, following professional video production standards.

---

### 5. **Gesture-Based Manual Control**

Enables intuitive human-robot interaction through **MediaPipe Hands**:
- **1 finger up (â˜ï¸)**: Move 100mm closer
- **5 fingers up (ğŸ–ï¸)**: Move 100mm back
- **OK sign (ğŸ‘Œ)**: Return to home position (300, 300, 450)mm
- Provides manual override of automatic distance control
- Implements 2-second cooldown to prevent accidental triggers

**User Experience Design:** Natural, non-verbal communication allows the subject to control the robot without breaking the flow of recording or leaving the frame. The OK sign gesture provides a quick way to reset the robot to a safe, known position.

---

### 6. **Safety-First Design**

Multiple safety layers protect both equipment and humans:
- **Immediate stop** when no face detected (prevents random movement)
- Workspace boundary enforcement (X: 100-600mm, Y: 10-590mm, Z: 250-650mm)
- Maximum single-axis movement limit (200mm per command)
- Conservative speed (60% max) and acceleration (100mm/sÂ²)
- Safety margins from physical limits to prevent arm-limit rejections

**Design Principle:** Robot should be predictable and safe for human-robot collaboration. All movements are constrained to a safe envelope, and the robot freezes if it loses sight of the subject.

---

### 7. **Monocular Vision Distance Estimation**

Uses face size as a proxy for distance without depth sensors:
- Based on similar triangles principle: `face_pixels = (face_width_mm Ã— focal_length) / distance_mm`
- Assumes average human face width of 150mm
- Enables 3D tracking with a single 2D camera

**Engineering Trade-off:** Sacrifices absolute distance accuracy for system simplicity and cost. Face size provides sufficient relative distance information for framing control without requiring expensive depth cameras.

---

### 8. **Responsive Control Architecture**

Decoupled processing for optimal performance:
- **Image processing**: 30 Hz (real-time face detection)
- **Control loop**: 5 Hz (robot movement commands)
- **Window updates**: 30 Hz (responsive UI)
- Asynchronous service calls prevent blocking
- Frame skipping during robot motion prevents blur-induced errors

**Architecture Rationale:** Different subsystems have different timing requirements. Vision needs high frame rate, robot control needs stability, and UI needs responsiveness. Decoupling allows each to operate at its optimal frequency.

---

### 9. **Video Recording with Clean Output**

Provides professional recording capability:
- Records clean video without UI overlays (bounding boxes, text, etc.)
- Maintains separate display image with tracking visualization
- MP4 format with timestamp-based filenames
- Toggle recording with 'v' key while system continues tracking

**Production Quality:** Separates operator feedback (debugging overlays) from final output, ensuring recorded footage is broadcast-ready without post-processing to remove UI elements.

---

### 10. **Direct Movement Control with Calibration**

Implements physics-based movement instead of empirical tuning:
```python
# Calculate visible width at target distance
visible_width_mm = 2 Ã— distance Ã— tan(FOV/2)

# Convert pixels to millimeters
mm_per_pixel = visible_width_mm / image_width

# Calculate robot movement
move_y_mm = -offset_x Ã— mm_per_pixel  # Horizontal
move_z_mm = -offset_y Ã— mm_per_pixel  # Vertical
```

This replaces trial-and-error scaling factors with mathematically derived conversions based on camera geometry.

**Engineering Advantage:** Calibration-based control is predictable, tunable, and transferable. Changing cameras or distances only requires updating FOV parameter, not re-tuning arbitrary scale factors.

---

### 11. **Position Smoothing Removal for Responsiveness**

**Recent optimization** (December 2025): Removed position averaging queue that introduced lag:
- **Original design**: Average last 5 positions for stability
- **Problem**: Created "stale data" lag with low-latency camera
- **Solution**: Use immediate detection results for faster response
- **Trade-off**: Slight jitter acceptable for better tracking responsiveness

**Iterative Design:** Initial assumption that smoothing improves performance was invalidated by testing. Fast camera (30 FPS) provides naturally smooth data, making additional smoothing counterproductive.

---

### 12. **ROS2 Integration for Robot Communication**

Leverages ROS2 ecosystem:
- **Topics**: `/image_raw` for camera feed (BEST_EFFORT QoS for low latency)
- **Services**: `send_script` for TM Script commands (PTP movements)
- **Bridge**: cv_bridge for OpenCV-ROS image conversion
- Enables modular architecture and easy extension

**Software Architecture:** ROS2 provides standardized interfaces, allowing the vision system, robot controller, and UI to operate as loosely-coupled modules. This enables independent development, testing, and potential replacement of components.

---

## Key Technical Innovation

The system's core innovation is **treating a robotic arm as a smart camera operator** rather than a fixed industrial manipulator. By combining:

1. **Real-time computer vision** (MediaPipe)
2. **Calibrated coordinate transformations** (camera geometry)
3. **Safety-focused control algorithms** (workspace limits, motion constraints)
4. **Human-robot interaction** (gesture recognition)

It creates an **autonomous vlogging assistant** that handles cinematography tasks traditionally requiring a human camera operator.

---

## Application Domains

This autonomous vlogger system is ideal for:

### **Content Creation**
- Solo YouTubers and streamers (no camera operator needed)
- Vloggers recording on-the-go with automatic framing
- Interview setups with dynamic subject tracking

### **Education**
- Virtual classroom recording (follows instructor)
- Lecture capture with automatic speaker tracking
- Online course production with professional framing

### **Corporate & Events**
- Conference presentation recording
- Product demonstration videos
- Corporate communications and town halls

### **Research & Development**
- Human-robot interaction studies
- Autonomous cinematography research
- Computer vision algorithm testing

---

## Design Philosophy Summary

The vlogger system embodies several key engineering principles:

1. **Human-Centered Design:** Robot adapts to human behavior, not vice versa
2. **Safety First:** Multiple layers of protection for human-robot collaboration
3. **Performance through Simplicity:** Monocular vision instead of complex depth sensing
4. **Calibration over Tuning:** Physics-based control instead of empirical parameters
5. **Modular Architecture:** ROS2 enables component independence and extensibility
6. **Real-Time Responsiveness:** 30 FPS tracking with sub-100ms latency
7. **Professional Output:** Clean recordings suitable for broadcast without post-processing

---

## Technical Challenges Overcome

### **Challenge 1: Low Frame Rate (0.3 FPS)**
- **Problem:** Built-in Techman camera required 3-second Vision_DoJob processing per frame
- **Solution:** Switched to USB webcam for 100Ã— improvement (30 FPS)
- **Impact:** Enabled real-time tracking instead of discrete, laggy movements

### **Challenge 2: Movement Overshoot**
- **Problem:** Direct pixel-to-mm mapping caused oscillation
- **Solution:** Camera FOV calibration and mm-per-pixel calculation
- **Impact:** Stable convergence without tuning arbitrary scale factors

### **Challenge 3: Robot Command Rejections**
- **Problem:** Commands at exact workspace boundaries rejected
- **Solution:** 10-20mm safety margins from physical limits
- **Impact:** 90% reduction in command rejection rate

### **Challenge 4: Incorrect Axis Mapping**
- **Problem:** Horizontal image offset drove depth axis (forward/backward)
- **Solution:** Corrected mapping - Image X â†’ Robot Y (side), not Robot X
- **Impact:** Proper orthogonal movement in correct directions

### **Challenge 5: Position Desync**
- **Problem:** Async service calls updated position before robot confirmation
- **Solution:** Conservative movement thresholds and workspace clamping
- **Status:** Partially mitigated; future improvement with FeedbackState subscription

---

## Future Directions

### **Short-Term Enhancements**
- Audio recording integration (FFmpeg subprocess)
- Real-time position feedback (subscribe to `/feedback_states`)
- Adaptive parameter tuning based on tracking performance

### **Medium-Term Goals**
- Predictive movement (Kalman filter)
- Intelligent cinematography (rule-of-thirds, shot variation)
- Multiple subject tracking and framing

### **Long-Term Vision**
- AI-powered director (ML-based shot composition)
- Natural language control (voice commands)
- Commercial applications (virtual classrooms, live events)

---

## System Workflow

This section describes the complete operational workflow of the vlogger system, from system startup to shutdown.

### **Workflow Overview Diagram**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYSTEM INITIALIZATION                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Launch tm_driver node (robot controller)                     â”‚
â”‚ 2. Enable "Listen Node" on TM5-900 teach pendant               â”‚
â”‚ 3. Start usb_cam node (camera feed)                             â”‚
â”‚ 4. Launch vlogger_control node (main controller)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CONTINUOUS OPERATION LOOP (Runtime)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ VISION PROCESSING (30 Hz - Image Callback)             â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Receive frame from /image_raw topic                â”‚    â”‚
â”‚  â”‚  2. Convert ROS Image â†’ OpenCV format (cv_bridge)      â”‚    â”‚
â”‚  â”‚  3. MediaPipe Face Mesh detection                      â”‚    â”‚
â”‚  â”‚     â””â”€> Extract face bounding box, center, size        â”‚    â”‚
â”‚  â”‚  4. MediaPipe Hands detection (if gesture mode)        â”‚    â”‚
â”‚  â”‚     â””â”€> Count extended fingers (1 or 5)                â”‚    â”‚
â”‚  â”‚  5. Draw overlays on display image                     â”‚    â”‚
â”‚  â”‚  6. Update latest_display_image (thread-safe)          â”‚    â”‚
â”‚  â”‚  7. Write clean frame to video (if recording)          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ CONTROL LOGIC (5 Hz - Control Loop Timer)              â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Check if human detected (safety check)             â”‚    â”‚
â”‚  â”‚     â””â”€> NO: Return immediately (do not move)           â”‚    â”‚
â”‚  â”‚     â””â”€> YES: Continue                                  â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  2. Calculate position offset from image center        â”‚    â”‚
â”‚  â”‚     offset_x = face_x - image_center_x                 â”‚    â”‚
â”‚  â”‚     offset_y = face_y - image_center_y                 â”‚    â”‚
â”‚  â”‚     distance = sqrt(offset_xÂ² + offset_yÂ²)             â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  3. Check if movement needed                           â”‚    â”‚
â”‚  â”‚     â€¢ Centering: distance > 20px threshold             â”‚    â”‚
â”‚  â”‚     â€¢ Distance: face_size outside 100Â±15px range       â”‚    â”‚
â”‚  â”‚     â€¢ Gesture: 1-finger or 5-finger command active     â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  4. Calculate new robot position                       â”‚    â”‚
â”‚  â”‚     â”Œâ”€> Coordinate transformation (calibrated)         â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ Horizontal: Image X â†’ Robot Y                â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ Vertical:   Image Y â†’ Robot Z                â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ Distance:   Face size â†’ Robot X              â”‚    â”‚
â”‚  â”‚     â”‚                                                   â”‚    â”‚
â”‚  â”‚     â”œâ”€> Apply mm-per-pixel conversion                  â”‚    â”‚
â”‚  â”‚     â”‚   mm_per_px = (2 Ã— D Ã— tan(FOV/2)) / width      â”‚    â”‚
â”‚  â”‚     â”‚   move_y = -offset_x Ã— mm_per_px                 â”‚    â”‚
â”‚  â”‚     â”‚   move_z = -offset_y Ã— mm_per_px                 â”‚    â”‚
â”‚  â”‚     â”‚                                                   â”‚    â”‚
â”‚  â”‚     â”œâ”€> Handle gesture commands                        â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ 1 finger: move_x += 100mm (closer)           â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ 5 fingers: move_x -= 100mm (backup)          â”‚    â”‚
â”‚  â”‚     â”‚   â€¢ OK sign: return to home (300,300,450)mm      â”‚    â”‚
â”‚  â”‚     â”‚                                                   â”‚    â”‚
â”‚  â”‚     â””â”€> Enforce workspace limits                       â”‚    â”‚
â”‚  â”‚         X: [100, 600]mm  Y: [10, 590]mm  Z: [250, 650]mm  â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  5. Check movement significance                        â”‚    â”‚
â”‚  â”‚     distance = sqrt(Î”xÂ² + Î”yÂ² + Î”zÂ²)                   â”‚    â”‚
â”‚  â”‚     â””â”€> If distance > 20mm: Send command               â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  6. Rate limiting (0.2s minimum between moves)         â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ROBOT COMMAND EXECUTION (Asynchronous)                 â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Generate PTP script command                        â”‚    â”‚
â”‚  â”‚     PTP("CPP", x, y, z, rx, ry, rz,                    â”‚    â”‚
â”‚  â”‚         speed=60%, accel=100mm/sÂ², blend=0)            â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  2. Send via send_script service (async)               â”‚    â”‚
â”‚  â”‚     â””â”€> Non-blocking call to tm_driver                 â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  3. Update internal position immediately               â”‚    â”‚
â”‚  â”‚     (optimistic update, assumes success)               â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  4. Service callback handles response                  â”‚    â”‚
â”‚  â”‚     â””â”€> Log warnings if robot rejects command          â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ UI UPDATE (30 Hz - Window Timer)                       â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  1. Acquire image lock (thread-safe)                   â”‚    â”‚
â”‚  â”‚  2. Copy latest_display_image                          â”‚    â”‚
â”‚  â”‚  3. Add frame freshness indicator                      â”‚    â”‚
â”‚  â”‚  4. Display via cv2.imshow()                           â”‚    â”‚
â”‚  â”‚  5. Handle keyboard input                              â”‚    â”‚
â”‚  â”‚     â€¢ 'q': Quit application                            â”‚    â”‚
â”‚  â”‚     â€¢ 'v': Toggle video recording                      â”‚    â”‚
â”‚  â”‚     â€¢ 's': Save current frame as PNG                   â”‚    â”‚
â”‚  â”‚     â€¢ 'r': Reset detection                             â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ (User presses 'q')
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SYSTEM SHUTDOWN                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Stop video recording (if active)                             â”‚
â”‚ 2. Release video writer                                         â”‚
â”‚ 3. Destroy OpenCV windows                                       â”‚
â”‚ 4. Shutdown ROS2 node                                           â”‚
â”‚ 5. Clean up resources                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **Detailed Workflow Steps**

#### **Phase 1: System Initialization**

**Step 1.1: Hardware & Driver Setup**
```bash
# Terminal 1: Launch TM robot driver
ros2 launch tm_driver tm_driver.launch.py robot_ip:=<ROBOT_IP>

# Enable Listen Node on robot teach pendant
# Settings â†’ Communication â†’ Listen Node (Port 5890)
```

**Step 1.2: Camera Initialization**
```bash
# Terminal 2: Launch USB camera (automatic with usb_cam package)
# Camera auto-detected at /dev/video0, publishes to /image_raw at 30 FPS
```

**Step 1.3: Vlogger Controller Launch**
```bash
# Terminal 3: Launch main control node
ros2 run vlogger_system vlogger_control
```

**Initialization Tasks:**
- Subscribe to `/image_raw` topic (BEST_EFFORT QoS)
- Create `send_script` service client
- Initialize MediaPipe Face Mesh and Hands models
- Create OpenCV display window
- Set robot to initial position (300, 300, 400)mm
- Start control loop timer (5 Hz)
- Start window update timer (30 Hz)

---

#### **Phase 2: Continuous Operation (Runtime Loop)**

This phase runs continuously until the user quits.

##### **2.1 Vision Processing Pipeline (30 Hz)**

**Triggered by:** New frame arrives on `/image_raw` topic

1. **Image Reception & Conversion**
   - Receive `sensor_msgs/Image` message
   - Convert to OpenCV format using cv_bridge
   - Optional resize (if target_width specified)
   - Update image center coordinates

2. **Face Detection (MediaPipe Face Mesh)**
   - Convert BGR â†’ RGB for MediaPipe
   - Process frame through face_mesh model
   - Extract 468 facial landmarks
   - Calculate bounding box:
     ```python
     x_min = min(all landmark x-coordinates)
     x_max = max(all landmark x-coordinates)
     y_min = min(all landmark y-coordinates)
     y_max = max(all landmark y-coordinates)
     ```
   - Compute face center and size:
     ```python
     face_width = x_max - x_min
     center_x = x_min + face_width / 2
     center_y = y_min + (y_max - y_min) / 2
     ```

3. **Hand Gesture Detection (MediaPipe Hands)**
   - Detect hand landmarks (21 points per hand)
   - Count extended fingers:
     ```python
     for each finger:
         if tip_y < pip_y:  # Tip above PIP joint
             fingers_up += 1
     ```
   - Recognize gestures:
     - 1 finger â†’ "CLOSER" command
     - 5 fingers â†’ "BACKUP" command
     - OK sign (thumb + index circle) â†’ "HOME" command
   - Apply 2-second cooldown to prevent re-triggering

4. **Position Update & Smoothing**
   - Store detected position: `(center_x, center_y, face_size)`
   - Update `current_human_pos` (no smoothing in current version)
   - If no face detected: Clear position data (safety)

5. **Overlay Rendering**
   - Draw face bounding box (green)
   - Draw center crosshair
   - Display tracking info (offset, face size)
   - Show gesture command (if active)
   - Draw hand landmarks (blue)
   - Add status panel (FPS, robot position, mode)
   - Add recording indicator (if recording)

6. **Image Storage & Recording**
   - Thread-safe copy to `latest_display_image`
   - Store clean frame to `latest_clean_image`
   - If recording: Write clean frame to video file

**Processing Time:** ~20-30ms per frame (maintains 30 FPS)

---

##### **2.2 Control Logic (5 Hz)**

**Triggered by:** Control loop timer (every 0.2 seconds)

1. **Safety Check: Face Detection**
   ```python
   if current_human_pos is None:
       return  # CRITICAL: Do not move without target
   ```
   - Robot immediately stops if no face in frame
   - Prevents random movements
   - Ensures predictable behavior

2. **Position Analysis**
   ```python
   human_x, human_y, face_size = current_human_pos

   # Calculate offset from image center
   offset_x = human_x - image_center_x
   offset_y = human_y - image_center_y
   distance = sqrt(offset_xÂ² + offset_yÂ²)
   ```

3. **Movement Decision Logic**
   ```python
   needs_centering = distance > 20  # pixels
   gesture_active = process_gesture_command()

   if needs_centering OR gesture_active:
       # Proceed to position calculation
   ```

4. **Coordinate Transformation**

   **Camera Calibration:**
   ```python
   FOV = 55Â°  # Camera horizontal field of view
   distance = 1000mm  # Assumed subject distance
   image_width = 640px

   # Calculate visible width at distance
   visible_width = 2 Ã— distance Ã— tan(FOV/2)

   # Pixels to millimeters conversion
   mm_per_pixel = visible_width / image_width
   ```

   **Axis Mapping:**
   ```python
   # Horizontal centering (Image X â†’ Robot Y)
   move_y = -offset_x Ã— mm_per_pixel Ã— scale

   # Vertical centering (Image Y â†’ Robot Z)
   move_z = -offset_y Ã— mm_per_pixel Ã— scale

   # Distance control (Face size â†’ Robot X)
   face_diff = face_size - target_face_size
   if abs(face_diff) > tolerance:
       move_x = -face_diff Ã— distance_gain
   ```

   **Gesture Override:**
   ```python
   if gesture == "closer":
       move_x += 100mm
   elif gesture == "backup":
       move_x -= 100mm
   elif gesture == "home":
       new_x, new_y, new_z = 300, 300, 450  # Return to home
   ```

5. **Safety Constraints**

   **Step Limiting:**
   ```python
   max_step = 200mm  # Per-axis limit
   move_x = clamp(move_x, -max_step, max_step)
   move_y = clamp(move_y, -max_step, max_step)
   move_z = clamp(move_z, -max_step, max_step)
   ```

   **Workspace Boundaries:**
   ```python
   new_x = clamp(new_x, 100, 600)    # Depth
   new_y = clamp(new_y, 10, 590)     # Side
   new_z = clamp(new_z, 250, 650)    # Height
   ```

6. **Movement Execution Decision**
   ```python
   move_distance = sqrt(Î”xÂ² + Î”yÂ² + Î”zÂ²)

   if move_distance > 20mm:  # Minimum movement
       if time_since_last_move > 0.2s:  # Rate limit
           send_robot_command(new_x, new_y, new_z)
   ```

---

##### **2.3 Robot Command Execution (Asynchronous)**

**Triggered by:** Control loop sends movement command

1. **Script Generation**
   ```python
   script = f'PTP("CPP",{x},{y},{z},{rx},{ry},{rz},60,100,0,false)'
   #        PTP( mode, x, y, z, rx,ry,rz, spd%,acc,blend,fine)
   ```

   **Parameters:**
   - `"CPP"`: Cartesian point-to-point mode
   - `(x, y, z)`: Target position in mm
   - `(rx, ry, rz)`: Orientation (90Â°, 0Â°, 50Â°) - camera pointing forward
   - `60`: Speed percentage (conservative for safety)
   - `100`: Acceleration (100 mm/sÂ²)
   - `0`: Blend radius (full stop at target)
   - `false`: Precision mode off

2. **Asynchronous Service Call**
   ```python
   request = SendScript.Request()
   request.script = script

   future = arm_client.call_async(request)
   future.add_done_callback(handle_response)
   ```
   - Non-blocking call prevents control loop stall
   - Robot controller receives command via TCP/IP
   - TM5-900 executes PTP movement

3. **Optimistic Position Update**
   ```python
   self.current_x = x
   self.current_y = y
   self.current_z = z
   ```
   - Position updated immediately (assumes success)
   - Enables next control loop calculation
   - **Known issue:** Can desync if robot rejects command

4. **Response Callback**
   ```python
   def handle_response(future):
       response = future.result()
       if not response.ok:
           log_warning("Robot rejected command")
   ```
   - Logs rejection reasons (workspace limit, speed error, etc.)
   - Does not block execution
   - Future improvement: Rollback position on failure

**Execution Time:**
- Service call: ~1-5ms (network latency)
- Robot movement: 0.5-2 seconds (depends on distance)

---

##### **2.4 User Interface Update (30 Hz)**

**Triggered by:** Window update timer (every 33ms)

1. **Image Acquisition (Thread-Safe)**
   ```python
   with image_lock:
       display_image = latest_display_image.copy()
       frame_age = current_time - latest_frame_timestamp
   ```

2. **Freshness Indicator**
   ```python
   color = green if age < 1s else orange if age < 3s else red
   draw_text(f"Frame age: {age:.2f}s", color)
   ```

3. **Display Update**
   ```python
   cv2.imshow('Vlogger View', display_image)
   ```

4. **Keyboard Input Handling**
   ```python
   key = cv2.waitKey(1) & 0xFF

   if key == ord('q'):
       shutdown_system()
   elif key == ord('v'):
       toggle_recording()
   elif key == ord('s'):
       save_frame()
   elif key == ord('r'):
       reset_detection()
   ```

**Window Content:**
- Live camera feed (640Ã—480, scaled to 1280Ã—960)
- Face tracking overlay (green box, center crosshair)
- Hand landmarks (blue skeleton)
- Tracking info (offset, face size, distance status)
- System status panel (FPS, robot position, mode)
- Recording indicator (ğŸ”´ REC timer)
- Keyboard shortcuts reminder

---

#### **Phase 3: Parallel Operations**

##### **3.1 Video Recording (Conditional)**

**Activated by:** User presses 'v' key

1. **Recording Start**
   ```python
   # Create recordings directory
   os.makedirs('recordings', exist_ok=True)

   # Generate timestamped filename
   timestamp = time.strftime("%Y%m%d_%H%M%S")
   filename = f'recordings/vlogger_{timestamp}.mp4'

   # Initialize video writer
   fourcc = cv2.VideoWriter_fourcc(*'mp4v')
   writer = cv2.VideoWriter(filename, fourcc, 30.0, (640, 480))
   ```

2. **Frame Writing (Per-Frame)**
   ```python
   if is_recording:
       writer.write(latest_clean_image)  # No overlays
   ```

   **Why clean image?**
   - Professional output without debugging info
   - Broadcast-ready footage
   - Overlays visible only in live view

3. **Recording Stop**
   ```python
   writer.release()
   duration = time.time() - recording_start_time
   log_info(f"Saved: {filename} ({duration:.1f}s)")
   ```

**File Format:**
- Codec: MP4V (H.264 compatible)
- Resolution: 640Ã—480 (native camera resolution)
- Frame rate: 30 FPS
- Location: `recordings/vlogger_YYYYMMDD_HHMMSS.mp4`

---

##### **3.2 Gesture Command Processing**

**Lifecycle of a Gesture Command:**

1. **Detection (Image Callback)**
   ```python
   fingers_up = count_fingers(hand_landmarks)

   if fingers_up == 1:
       gesture = "CLOSER"
       last_gesture_time = current_time
   elif fingers_up == 5:
       gesture = "BACKUP"
       last_gesture_time = current_time
   elif is_ok_sign(hand_landmarks):
       gesture = "HOME"
       last_gesture_time = current_time
   ```

2. **Cooldown Period (2 seconds)**
   ```python
   # During cooldown:
   # - No new gestures detected
   # - Current gesture remains active
   # - Robot executes command

   if time_since_gesture < 2.0:
       return last_gesture  # Keep active
   ```

3. **Command Application (Control Loop)**
   ```python
   if gesture == "closer":
       new_x += 100mm
       auto_distance_adjust = False  # Disable auto
   elif gesture == "backup":
       new_x -= 100mm
       auto_distance_adjust = False
   elif gesture == "home":
       new_x, new_y, new_z = 300, 300, 450  # Return to home position
       auto_distance_adjust = True  # Re-enable auto after home
   ```

4. **Auto-Resume (5 seconds)**
   ```python
   if time_since_gesture > 5.0:
       auto_distance_adjust = True  # Re-enable
   ```

**User Experience Flow:**
1. Subject raises 1 finger â†’ Robot sees gesture
2. Robot logs "ğŸ‘† Gesture detected: MOVE CLOSER"
3. Robot moves 100mm forward over ~1 second
4. 2-second cooldown prevents accidental re-trigger
5. After 5 seconds, automatic distance control resumes

**Home Position Reset:**
1. Subject shows OK sign (ğŸ‘Œ) â†’ Robot sees gesture
2. Robot logs "ğŸ‘Œ Gesture detected: RETURN HOME"
3. Robot smoothly returns to home position (300, 300, 450)mm
4. Automatic distance control re-enabled immediately
5. Useful for resetting after manual adjustments or camera angle changes

---

#### **Phase 4: System Shutdown**

**Triggered by:** User presses 'q' or Ctrl+C

1. **Graceful Shutdown Sequence**
   ```python
   try:
       # Stop recording if active
       if is_recording:
           stop_recording()

       # Clean up resources
       controller.destroy_node()
       rclpy.shutdown()
       cv2.destroyAllWindows()

   except KeyboardInterrupt:
       log_info("Emergency shutdown...")
   ```

2. **Resource Cleanup**
   - Release video writer (flush buffer to disk)
   - Close OpenCV windows
   - Disconnect from robot (service client)
   - Destroy ROS2 node
   - Shutdown ROS2 context

3. **Robot Final State**
   - Robot remains at last commanded position
   - Listen Node remains active for next session
   - No automatic "home" position (safety)

---

### **Timing & Performance Summary**

| Component | Frequency | Latency | Notes |
|-----------|-----------|---------|-------|
| **Camera Feed** | 30 Hz (33ms) | ~5ms | USB webcam to /image_raw |
| **Face Detection** | 30 Hz | ~15ms | MediaPipe CPU inference |
| **Hand Detection** | 30 Hz | ~10ms | MediaPipe CPU inference |
| **Control Loop** | 5 Hz (200ms) | ~2ms | Position calculation |
| **Robot Command** | 0.2-5 Hz | 1-5ms | Service call (async) |
| **Robot Movement** | As needed | 0.5-2s | Physical motion time |
| **UI Update** | 30 Hz (33ms) | ~2ms | Window refresh |
| **Total Latency** | - | ~50ms | Detection to command |

**Performance Characteristics:**
- **Responsive:** 50ms end-to-end latency (perception to action)
- **Stable:** 5 Hz control loop prevents jitter
- **Smooth:** 30 FPS vision provides natural tracking
- **Safe:** Immediate stop on lost detection (<200ms)

---

### **Data Flow Summary**

```
Camera â†’ USB â†’ /image_raw â†’ VloggerController â†’ MediaPipe â†’ Position
                                    â†“
                              Control Logic â†’ Coordinate Transform
                                    â†“
                          PTP Command â†’ send_script Service
                                    â†“
                              TM Driver â†’ Robot
                                    â†“
                          Physical Movement â†’ Camera sees result
                                    â†“
                              [Feedback Loop]
```

---

### **Error Handling & Recovery**

**Scenario 1: Face Lost During Tracking**
- **Detection:** `current_human_pos = None`
- **Action:** Robot stops immediately
- **Recovery:** Resume tracking when face reappears

**Scenario 2: Robot Command Rejected**
- **Detection:** Service response `ok = False`
- **Action:** Log warning, skip position update
- **Recovery:** Next control cycle recalculates

**Scenario 3: Camera Feed Interrupted**
- **Detection:** No frames received for >8 seconds
- **Action:** Log timeout warning
- **Recovery:** Wait for feed to resume

**Scenario 4: Workspace Boundary Hit**
- **Detection:** Calculated position exceeds limits
- **Action:** Clamp to boundary, log warning
- **Recovery:** Move inward when face repositions

---

### **State Machine View**

```
IDLE (No Face)
    â”‚
    â”‚ [Face Detected]
    â–¼
TRACKING (Centering + Distance)
    â”‚
    â”œâ”€â”€[Gesture: 1 Finger]â”€â”€> MANUAL_CLOSER (2s)
    â”‚                              â”‚
    â”‚                              â”‚ [Timeout 5s]
    â”‚                              â–¼
    â”œâ”€â”€[Gesture: 5 Fingers]â”€> MANUAL_BACKUP (2s)
    â”‚                              â”‚
    â”‚                              â”‚ [Timeout 5s]
    â”‚                              â–¼
    â”œâ”€â”€[Gesture: OK Sign]â”€â”€â”€â”€> RETURN_HOME (immediate)
    â”‚                              â”‚
    â”‚                              â”‚ [Auto re-enabled]
    â”‚                              â–¼
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> TRACKING (Auto-resume)

    [Face Lost]
    â”‚
    â–¼
IDLE (Robot Stopped)
```

---

### **Configuration Parameters**

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `target_face_size` | 120px | Desired face height for framing |
| `face_size_tolerance` | 20px | Acceptable range (100-140px) |
| `centering_threshold` | 20px | Minimum offset to trigger move |
| `min_movement` | 20mm | Ignore moves smaller than this |
| `max_single_axis_step` | 200mm | Safety limit per axis |
| `control_loop_rate` | 5 Hz | Position update frequency |
| `camera_fov` | 55Â° | Horizontal field of view |
| `speed_percent` | 60% | Robot movement speed |
| `acceleration` | 100mm/sÂ² | Robot acceleration |
| `gesture_cooldown` | 2.0s | Debounce time for gestures |

---

## Conclusion

The TM5-900 Indoor Vlogger System demonstrates that **intelligent robotics can democratize professional video production**. By automating the camera operator role, it enables:

- Solo creators to produce professional content
- Educators to create engaging virtual classrooms
- Researchers to explore autonomous cinematography

The system's combination of computer vision, calibrated control, and safety-focused design creates a practical, accessible solution for autonomous video recording that maintains the quality standards of human-operated cameras.

---

**Document Version:** 1.0
**Last Updated:** December 9, 2025
**Repository:** https://github.com/misuhsieh001/Robotics_Final

**Related Documentation:**
- `README.md` - User guide and quick start
- `PROJECT_DOCUMENTATION.md` - Complete technical documentation
- `FIX_DOCUMENTATION.md` - Development history and troubleshooting
