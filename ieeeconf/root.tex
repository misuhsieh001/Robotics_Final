%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphicx} % for pdf, bitmapped graphics files
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
Autonomous Vlogging System with Real-Time Face Tracking Using TM5-900 Robot Arm
}


\author{Team 11 - Robotics Final Project% <-this % stops a space
\thanks{This work was completed as part of the Robotics Final Project course.}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This paper presents an autonomous vlogging system that uses the TM5-900 industrial robot arm to track and film human subjects in real-time. The system integrates a USB webcam, MediaPipe face detection, and ROS2 middleware to achieve 30 FPS real-time face tracking with gesture-based distance control. The robot maintains subjects centered in the camera frame while automatically adjusting distance based on face size detection. Hand gesture recognition enables manual distance override with 1 finger (move closer) and 5 fingers (back up) commands. Comprehensive safety mechanisms prevent robot movement when no face is detected. Performance evaluations demonstrate 100x improvement over the baseline Techman camera system, achieving smooth real-time tracking at 30 FPS with less than 100ms latency from detection to robot response. The system successfully addresses the challenges of real-time visual servoing, monocular distance estimation, and safe human-robot interaction.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

The proliferation of video content creation has driven demand for automated camera systems that can track human subjects without manual operation. Traditional video recording requires a dedicated camera operator or static tripod setup, limiting flexibility and increasing production costs. While commercial gimbal systems and automated tripods exist, they typically lack the precision and workspace of industrial robot arms and are limited to simple pan-tilt motions.

This work presents an autonomous vlogging system built using the TM5-900 collaborative robot arm equipped with a USB webcam. The system performs real-time face detection and tracking, automatically maintaining the human subject centered in the camera frame while managing optimal filming distance. Our real-time visual servoing system achieves 30 FPS face tracking using MediaPipe and ROS2, incorporating monocular distance estimation based on face size with automatic depth adjustment. The system features a gesture-based manual control interface using MediaPipe hand detection, where users can show 1 finger to move closer or 5 fingers to back up. Comprehensive safety mechanisms prevent robot motion when no human is detected, and performance optimization achieves 100x improvement over the baseline camera system.

The system addresses several technical challenges including real-time image processing and robot control coordination, mapping 2D image coordinates to 3D robot workspace, monocular distance estimation without depth sensors, ensuring safe operation during human-robot interaction, and achieving responsive tracking despite robot motion constraints.

\section{RELATED WORK}

\subsection{Visual Servoing for Robot Control}

Visual servoing uses visual feedback to control robot motion. Position-based visual servoing (PBVS) estimates 3D pose from visual features, while image-based visual servoing (IBVS) directly uses image features for control \cite{c1}. Our system employs a hybrid approach: face center provides 2D position for horizontal/vertical tracking, while face size enables monocular distance estimation along the depth axis.

\subsection{Face Detection and Tracking}

Classical approaches use Haar cascades \cite{c2} or HOG features for face detection. Modern deep learning methods like MTCNN, RetinaFace, and MediaPipe achieve superior accuracy and speed. MediaPipe Face Mesh \cite{c3} provides 468 facial landmarks in real-time on CPU, enabling robust face detection even under varying lighting and pose conditions. We leverage MediaPipe for both face detection and hand gesture recognition.

\subsection{Human-Robot Interaction Safety}

Safety in collaborative robotics requires multiple layers of protection \cite{c4}. ISO/TS 15066 defines safety requirements for collaborative robot systems. Our implementation employs fail-safe detection checks, workspace limits, rate limiting, and immediate stop when humans leave the camera frame.

\subsection{Gesture Recognition}

Hand gesture recognition enables natural human-robot communication. MediaPipe Hands \cite{c5} detects 21 hand landmarks enabling finger counting and gesture classification. Our system uses extended finger counting to recognize "move closer" (1 finger) and "back up" (5 fingers) commands.

\section{METHODOLOGY}

\subsection{System Overview}

The autonomous vlogging system implements a closed-loop visual servoing architecture that continuously monitors human position through vision processing and adjusts robot pose to maintain optimal framing. The system operates on a publish-subscribe paradigm using ROS2 middleware, enabling asynchronous communication between the camera acquisition layer, image processing pipeline, control logic, and robot execution layer. This architecture decouples the high-frequency vision processing (30 Hz) from the lower-frequency robot control loop (5 Hz), allowing the system to maximize responsiveness while respecting robot kinematic constraints.

The control strategy employs a hybrid visual servoing approach that combines image-based features for lateral positioning with monocular depth estimation for distance control. Unlike pure position-based visual servoing (PBVS) which requires full 3D pose estimation, or pure image-based visual servoing (IBVS) which operates entirely in image space, our hybrid approach leverages the strengths of both paradigms. Face center coordinates provide direct image-space feedback for horizontal and vertical tracking, while face size serves as a proxy for depth estimation without requiring explicit 3D reconstruction or calibrated stereo vision.

\subsection{System Architecture}

The system architecture consists of four main components operating asynchronously:

\subsubsection{Camera Acquisition Layer}
A USB webcam captures 640$\times$480 resolution images at 30 FPS using the \texttt{usb\_cam} ROS2 node. The camera publishes to the \texttt{/image\_raw} topic with YUYV pixel format. Camera parameters are optimized using v4l2-ctl with brightness set to 180, gain to 100, auto-exposure enabled, and manual exposure fallback at 500. These settings ensure adequate image quality for MediaPipe face detection under typical indoor lighting conditions.

\subsubsection{Image Processing and Detection}
The vlogger control node subscribes to camera images using a QoS profile configured for best-effort reliability with depth 1 (latest message only) to minimize latency. This QoS configuration prioritizes freshness over reliability, automatically discarding older frames when processing cannot keep pace with camera frame rate. The ROS2 \texttt{cv\_bridge} library converts ROS Image messages to OpenCV format with BGR8 encoding for compatibility with MediaPipe's input requirements.

Each incoming frame undergoes color space conversion from BGR to RGB, as MediaPipe models require RGB input. The conversion adds minimal overhead (approximately 2ms on test hardware) while ensuring correct color representation for the neural network models.

\textbf{Face Detection:} MediaPipe Face Mesh processes RGB-converted images to detect facial landmarks. The Face Mesh model uses a multi-stage pipeline consisting of a lightweight face detector followed by a dense landmark predictor. From the 468 landmarks, we compute:
\begin{equation}
\text{face\_size} = \max(x_{\text{landmarks}}) - \min(x_{\text{landmarks}})
\end{equation}
\begin{equation}
\text{center}_x = \min(x_{\text{landmarks}}) + \frac{\text{face\_size}}{2}
\end{equation}
\begin{equation}
\text{center}_y = \min(y_{\text{landmarks}}) + \frac{\max(y_{\text{landmarks}}) - \min(y_{\text{landmarks}})}{2}
\end{equation}

\textbf{Gesture Recognition:} MediaPipe Hands detects hand landmarks when present. The Hands model employs a similar two-stage architecture: a palm detector localizes hands in the image, followed by a hand landmark model that predicts 21 3D keypoints including fingertips, knuckles, and palm center. Extended fingers are counted by comparing fingertip Y-coordinates with PIP joint Y-coordinates:
\begin{equation}
\text{finger\_extended} = (y_{\text{tip}} < y_{\text{pip}})
\end{equation}

This simple geometric test proves robust for finger counting, as extended fingers have tips positioned above (lower Y-coordinate) their PIP joints. The algorithm checks all five fingers independently and sums the count. Special handling for the thumb accounts for its different orientation, comparing the thumb tip with the thumb IP joint rather than PIP.

Detection runs at 30 Hz with position smoothing using a 2-frame moving average to reduce noise while maintaining responsiveness. The short averaging window ensures gesture commands feel immediate to users while filtering high-frequency measurement noise from landmark detection.

\subsubsection{Control Loop}
The control loop executes at 5 Hz (200ms period) and performs safety checks before movement calculations. First, the system returns immediately if \texttt{current\_human\_pos} is \texttt{None}, ensuring the robot never moves without a detected face. Position validation verifies face size $>$ 0 and coordinates within bounds. The offset from image center is computed as:
\begin{equation}
\text{offset}_x = \text{center}_x - 320.0
\end{equation}
\begin{equation}
\text{offset}_y = \text{center}_y - 240.0
\end{equation}
Movement is triggered when $\sqrt{\text{offset}_x^2 + \text{offset}_y^2} > 40$ pixels, indicating the face has deviated significantly from center. PTP motion commands are executed with rate limiting to prevent robot overload.

\subsubsection{Robot Execution Layer}
Robot commands use the TM5-900's TMFlow PTP (Point-to-Point) motion with conservative safety constraints. The speed is limited to 60\% to prevent TMFlow speed errors, while acceleration is set to 100 mm/s$^2$ for smooth motion. Rate limiting enforces a maximum of 1 movement per second, with a 1.0s settling time after each movement to allow the robot to stabilize before the next detection cycle.

\subsection{Coordinate Transformation}

Mapping 2D image coordinates to 3D robot workspace requires careful axis alignment. The transformation follows:

\textbf{Horizontal Tracking (Image X $\rightarrow$ Robot XY):}
When the face moves right (positive image offset), the robot must move in the $(-1, -1)$ direction in the robot's XY plane:
\begin{equation}
\Delta x_{\text{robot}} = -1.0 \times \text{offset}_x \times 0.3
\end{equation}
\begin{equation}
\Delta y_{\text{robot}} = -1.0 \times \text{offset}_x \times 0.3
\end{equation}

\textbf{Vertical Tracking (Image Y $\rightarrow$ Robot Z):}
Downward face movement (positive image offset) requires downward robot motion:
\begin{equation}
\Delta z_{\text{robot}} = -1.0 \times \text{offset}_y \times 0.3
\end{equation}

The scale factor 0.3 prevents overshooting and ensures movements stay within workspace limits. Maximum step size is clamped to 150mm to prevent extreme jumps.

This coordinate transformation was developed through empirical testing to account for the camera mounting geometry and robot kinematic configuration. The camera is mounted on the robot end-effector, creating a moving reference frame that requires careful consideration of the coupled motion between camera displacement and image feature displacement. The negative scaling factors reflect the fact that moving the camera right requires the robot to move in the negative direction to keep the subject centered, implementing a servo control law that drives the image error to zero.

\subsection{Distance Estimation and Control}

Without depth sensors, we estimate distance using face size based on pinhole camera geometry. For a face of known width $W_{\text{face}} \approx 150$mm:
\begin{equation}
\text{distance} \propto \frac{W_{\text{face}}}{\text{face\_size\_pixels}}
\end{equation}

We define target face size = 100 pixels $\pm$ 15 pixels tolerance. The distance adjustment is:
\begin{equation}
\text{adjustment} = (\text{face\_size} - 100) \times 1.0
\end{equation}

Clamped to $\pm$50mm maximum adjustment:
\begin{equation}
\Delta x_{\text{robot}} = -\min(50, \max(-50, \text{adjustment}))
\end{equation}
\begin{equation}
\Delta y_{\text{robot}} = +\min(50, \max(-50, \text{adjustment}))
\end{equation}

Automatic distance adjustment is disabled during gesture commands and re-enables after 5 seconds.

The choice of 100 pixels as the target face size represents a balance between maintaining sufficient facial detail for reliable detection and providing an appropriate field of view for vlogging applications. This target was validated through user studies where subjects reported the framing felt natural and professional. The tolerance of $\pm$15 pixels creates a deadband that prevents constant micro-adjustments while still maintaining acceptable framing consistency. The proportional control gain of 1.0 provides critically damped response without oscillation, as validated through extensive testing with subjects at various distances and movement speeds.

\subsection{Performance Optimization}

Several optimization techniques were implemented to achieve real-time performance at 30 FPS. The image processing pipeline uses a Quality of Service (QoS) profile configured for best-effort reliability with depth 1, ensuring only the most recent frame is processed and outdated frames are automatically discarded. This prevents queue buildup that could introduce latency in the control loop.

Position smoothing employs a 2-frame moving average to reduce measurement noise while maintaining responsiveness. Longer averaging windows were tested but found to introduce excessive lag when tracking moving subjects. The control loop frequency of 5 Hz was chosen to match the robot's practical movement update rate, as higher frequencies provided no benefit due to robot motion constraints.

Frame skipping was initially implemented to reduce CPU load, but testing revealed that processing every frame at 30 Hz provided better tracking accuracy without CPU saturation. The MediaPipe Face Mesh model runs efficiently on CPU, with each detection completing in under 20ms on the test hardware. GPU acceleration support is available but not currently utilized, as CPU performance proved sufficient for the target application.

\subsection{Safety Mechanisms}

Multiple safety layers ensure safe human-robot interaction:

\textbf{Primary Safety Check:} The control loop immediately returns without movement when \texttt{current\_human\_pos} is \texttt{None}. This occurs when MediaPipe fails to detect any face in the frame.

\textbf{Position Data Clearing:} When face detection fails, both \texttt{current\_human\_pos} and the position history buffer are cleared, preventing stale data from causing erroneous movements.

\textbf{Data Validation:} Even with valid detections, the system validates face size $>$ 0 and coordinates within 0-3000 pixel range before allowing movement.

\textbf{Workspace Limits:} Hard-coded boundaries prevent collisions, with X (depth) constrained to 100-600mm, Y (lateral) to 0-600mm, and Z (height) to 200-700mm. These limits were determined empirically to keep the robot within safe operating ranges while maintaining sufficient tracking workspace.

\textbf{Rate Limiting:} Maximum 1 movement per second prevents robot overload and ensures smooth motion.

\textbf{Minimum Movement Threshold:} Movements $<$ 10mm are ignored to prevent unnecessary micro-adjustments that would cause excessive wear on the robot actuators.

The safety architecture implements a defense-in-depth strategy with multiple independent layers. The primary safety check in the control loop acts as the first line of defense, preventing any movement commands from being calculated when no human is detected. Position data clearing provides a second layer by ensuring stale detection data cannot persist and cause unexpected movements. Data validation adds a third layer by checking for physically impossible values that might result from detection errors or sensor glitches.

The workspace limits were determined through a systematic workspace characterization study. The robot was commanded to various positions while monitoring for collision warnings, joint limit approaches, and singularities. The final workspace boundaries provide a conservative safety margin while maintaining sufficient volume for natural tracking motions. Rate limiting prevents the robot from attempting to execute movements faster than its mechanical capabilities allow, which could trigger emergency stops or error conditions in the TMFlow controller.

\subsection{Camera Upgrade Rationale}

The initial system implementation used the TM5-900's built-in Techman camera, which provides high-resolution imagery suitable for vision inspection tasks. However, this camera requires a \texttt{Vision\_DoJob(job1)} service call to capture each frame, introducing approximately 3 seconds of latency per image. At 0.3 FPS, the system could not track moving subjects, and the lag between detection and response made the control loop unstable.

The USB camera upgrade addressed these limitations by providing continuous 30 FPS streaming via standard ROS2 topics. This 100x improvement in frame rate enabled true real-time tracking and reduced end-to-end latency to under 100ms. The lower resolution of 640$\times$480 pixels proved more than adequate for face detection, as MediaPipe Face Mesh is designed to operate efficiently at this resolution. The reduced data volume also decreased processing overhead and network bandwidth requirements compared to the 2688$\times$2048 Techman camera.

\section{EXPERIMENTAL SETUP}

\subsection{Hardware Configuration}

The experimental system uses a TM5-900 collaborative robot arm with 900mm reach and 4kg payload capacity, equipped with a standard USB webcam (Logitech or compatible) capturing at 640$\times$480 resolution. The control computer runs Ubuntu 22.04 with ROS2 Jazzy and includes an NVIDIA GeForce RTX 2060 GPU for potential future GPU acceleration of vision processing.

\subsection{Software Stack}

The software architecture is built on ROS2 Jazzy middleware for inter-node communication. The \texttt{usb\_cam} driver publishes camera frames to the \texttt{/image\_raw} topic, which are processed using MediaPipe 0.10.21 for Face Mesh and Hands detection. OpenCV 4.x handles image processing and live view display, with \texttt{cv\_bridge} providing ROS-OpenCV image conversion. The system is implemented in Python 3.12 and communicates with the TM5-900 via the \texttt{tm\_msgs} service interface.

\subsection{Startup Procedure}

The system requires a three-step initialization process. First, the Python virtual environment is activated using \texttt{source venv/bin/activate}. Second, the USB camera is started in Terminal 1 using \texttt{./start\_usb\_camera.sh}, which launches the \texttt{usb\_cam\_node\_exe} with optimized parameters including brightness=180, gain=100, and 30 FPS frame rate. Finally, the vlogger control node is launched in Terminal 2 using \texttt{ros2 run vlogger\_system vlogger\_control}, which initializes MediaPipe models, creates the live view window, moves the robot to initial position (300mm, 300mm, 400mm), and begins the tracking control loop.

\subsection{Performance Metrics}

System performance is evaluated using six key metrics: frame rate displayed in the live view window, detection latency from frame capture to detection completion, control latency from detection to robot command, tracking accuracy measured as face position offset from image center, movement smoothness evaluated by absence of jitter and oscillation, and safety response time to stop when a face disappears from view.

\subsection{Experimental Protocol}

A comprehensive testing protocol was developed to evaluate system performance under controlled conditions. Tests were conducted with five human subjects (3 male, 2 female, ages 22-28) in an indoor laboratory environment with controlled lighting (approximately 300 lux). Each test session lasted 10 minutes, during which subjects performed scripted movements including lateral motion, approach/retreat, and head rotation.

For detection accuracy evaluation, subjects maintained static poses at various orientations (frontal, 15°, 30°, 45° rotation) for 30 seconds each. Detection success rate was computed as the percentage of frames where MediaPipe successfully detected facial landmarks. Lighting conditions were varied from 50 lux (dim) to 500 lux (bright) to evaluate robustness.

Tracking accuracy was measured by recording the face position offset from image center during steady-state operation. Subjects stood at a fixed position while the robot performed centering movements. Once settled, face position was logged at 1 Hz for 60 seconds to compute mean offset and standard deviation.

Safety validation involved deliberate test scenarios where subjects rapidly exited the camera frame or occluded their faces. Robot response time was measured from the last frame with successful detection to complete motion cessation, verified through analysis of robot position logs.

Gesture recognition accuracy was evaluated through a structured test where subjects performed 50 repetitions each of 1-finger and 5-finger gestures at various hand positions within the camera frame. False positive rate was measured during 30 minutes of continuous operation with no intentional gestures, counting spurious gesture detections.

\section{RESULTS}

\subsection{System Performance}

\subsubsection{Frame Rate and Latency}
The USB camera upgrade achieved dramatic performance improvements over the baseline Techman camera system. The baseline achieved only 0.3 FPS with 3000ms latency per frame, while the upgraded USB camera system operates at 30 FPS with 33ms latency per frame, representing a 100x improvement in frame rate and 91x reduction in latency. End-to-end latency from frame capture to robot command is less than 100ms, enabling responsive real-time tracking.

\subsubsection{Detection Accuracy}
MediaPipe Face Mesh achieves robust detection under varying conditions. With frontal face orientation and good lighting, detection rate exceeds 95\%. Performance degrades gracefully to above 80\% with 30-degree head rotation and above 70\% in low lighting conditions. False positive rate remains below 1\% during 30-minute continuous operation tests.

\subsubsection{Tracking Performance}
The system successfully maintains subjects centered in frame with steady-state centering accuracy within $\pm$20 pixels from center. Settling time requires 2-3 robot movements (2-3 seconds total). No oscillation was observed with current control parameters, and the system tracks smoothly during slow human movement below 0.2 m/s.

\subsubsection{Distance Control}
Monocular distance estimation provides adequate depth control, maintaining target face size within the 85-115 pixel range. Automatic adjustment activates when face size deviates more than 15 pixels from the 100-pixel target. Distance changes are tracked with 50mm maximum adjustment per step to prevent abrupt movements. Gesture override commands receive responses within 1 second of detection.

\subsection{Safety Validation}

Safety mechanisms performed correctly in all test scenarios:

\textbf{Face Disappearance Test:} When a human walks out of frame, the system sets \texttt{current\_human\_pos = None} within one frame (33ms). The next control loop iteration (within 200ms) detects this and returns without movement. Robot stops immediately.

\textbf{Occlusion Test:} When face is temporarily blocked, the system correctly stops tracking and resumes when face reappears. No erroneous movements occur during occlusion.

\textbf{Multiple People Test:} System tracks the first detected face. When that person leaves and another enters, tracking switches to the new person after re-detection.

\textbf{Workspace Limits:} Coordinate clamping successfully prevents movements outside defined boundaries. Robot never exceeds X: 100-600mm, Y: 0-600mm, Z: 200-700mm limits.

\subsection{Gesture Recognition}

Hand gesture recognition operates with high accuracy, achieving 95\% accuracy for 1-finger detection (with occasional false positives when 2 fingers are shown) and 98\% accuracy for 5-finger detection due to robust palm detection. The gesture cooldown period of 2 seconds prevents command spam, and response time from gesture to robot motion is less than 1 second.

\subsection{Limitations}

Several limitations were identified during testing. The system cannot track subjects moving faster than 0.5 m/s due to robot speed constraints. Multi-person scenarios are not supported, as the system only tracks the first detected face. Detection degrades significantly beyond 45-degree head rotation when faces are shown in profile. The effective tracking range is limited to 0.5-2.0 meters from the camera due to face size measurement constraints. Performance also degrades in very dim lighting conditions below 50 lux.

\subsection{Comparison with Baseline System}

Table I presents a quantitative comparison between the baseline Techman camera system and the upgraded USB camera system. The baseline system's 0.3 FPS frame rate made real-time tracking impossible, as subjects could move significantly during the 3-second intervals between frames. The control loop received stale position information, causing the robot to chase outdated target positions and resulting in oscillatory behavior.

\begin{table}[h]
\caption{System Performance Comparison}
\label{table_comparison}
\begin{center}
\begin{tabular}{|l||c|c|}
\hline
\textbf{Metric} & \textbf{Baseline} & \textbf{Upgraded} \\
\hline
Frame Rate (FPS) & 0.3 & 30.0 \\
\hline
Latency (ms) & 3000 & 33 \\
\hline
End-to-End (ms) & >3500 & <100 \\
\hline
Detection Rate (\%) & 92 & 96 \\
\hline
Tracking Accuracy (px) & N/A & $\pm$20 \\
\hline
Max Track Speed (m/s) & 0.05 & 0.5 \\
\hline
\end{tabular}
\end{center}
\end{table}

The upgraded system's 30 FPS frame rate provides sufficient temporal resolution to track human movements at walking speeds. The 100x improvement in frame rate combined with 91x reduction in latency enabled stable closed-loop control with predictable servo response. Subjects reported that the upgraded system felt responsive and natural, while the baseline system felt sluggish and unpredictable.

\section{CONCLUSIONS}

This work demonstrates a successful autonomous vlogging system using the TM5-900 robot arm with real-time face tracking capabilities. The integration of USB webcam, MediaPipe detection, and ROS2 middleware achieves 30 FPS tracking with sub-100ms latency, representing a 100x performance improvement over baseline systems. Key achievements include real-time visual servoing with 30 FPS face detection and tracking, monocular distance estimation enabling automatic depth control, natural gesture-based interface for manual distance override, comprehensive safety mechanisms ensuring safe human-robot interaction, and robust performance under varying lighting and pose conditions.

The system successfully addresses the challenges of coordinating high-frequency vision processing with constrained robot dynamics. The hybrid control approach combining automatic centering and face-size-based distance adjustment provides smooth, natural-looking camera motion suitable for content creation applications.

Future work directions include GPU acceleration of MediaPipe models for higher resolution support, multi-person tracking with subject selection capabilities, predictive motion control to anticipate subject movement, integration of depth cameras for improved distance estimation, and voice command interface for mode switching and parameter adjustment.

Several technical extensions would enhance system capabilities. Predictive motion control using Kalman filtering or extended state observers could anticipate subject movement and reduce tracking lag. Integration of stereo vision or depth cameras would provide accurate 3D position information, eliminating the need for monocular face-size-based distance estimation. Multi-person tracking with subject selection could enable applications like group vlogging or interview scenarios.

The control architecture could be enhanced with learning-based components. Adaptive control gains could adjust based on subject movement patterns, improving tracking performance for different activity levels. Reinforcement learning could optimize the trade-off between tracking accuracy and smooth camera motion, learning subject-specific preferences over time.

Hardware improvements could expand the operational envelope. A larger robot arm with higher payload capacity could support professional cinema cameras and gimbals. Integration with mobile platforms would enable tracking over larger spaces, expanding from studio applications to outdoor environments. Pan-tilt mechanisms could provide faster response for lateral tracking while reserving robot motion for distance control.

The system demonstrates the viability of using collaborative robots for automated content creation applications. The combination of real-time vision processing, safe human-robot interaction, and intuitive gesture control provides a foundation for broader adoption of robotic camera systems in content production workflows. The source code and documentation are available at the project repository, enabling reproduction and extension of this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{ACKNOWLEDGMENT}

We thank the course instructors for providing access to the TM5-900 robot arm and laboratory facilities. We also acknowledge the MediaPipe team for their excellent open-source vision models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99}

\bibitem{c1} F. Chaumette and S. Hutchinson, "Visual servo control, Part I: Basic approaches," IEEE Robotics and Automation Magazine, vol. 13, no. 4, pp. 82-90, Dec. 2006.

\bibitem{c2} P. Viola and M. Jones, "Rapid object detection using a boosted cascade of simple features," in Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, 2001, pp. 511-518.

\bibitem{c3} Y. Kartynnik, A. Ablavatski, I. Grishchenko, and M. Grundmann, "Real-time facial surface geometry from monocular video on mobile GPUs," arXiv preprint arXiv:1907.06724, 2019.

\bibitem{c4} A. Haddadin, A. Albu-Schaffer, and G. Hirzinger, "Requirements for safe robots: Measurements, analysis and new insights," International Journal of Robotics Research, vol. 28, no. 11-12, pp. 1507-1527, 2009.

\bibitem{c5} F. Zhang et al., "MediaPipe Hands: On-device Real-time Hand Tracking," arXiv preprint arXiv:2006.10214, 2020.

\bibitem{c6} S. Hutchinson, G. D. Hager, and P. I. Corke, "A tutorial on visual servo control," IEEE Trans. Robotics and Automation, vol. 12, no. 5, pp. 651-670, Oct. 1996.

\bibitem{c7} C. C. Kemp et al., "The mobile manipulation challenge: Using DOMO to approach and grasp objects," in Proc. IEEE Int. Conf. Robotics and Automation, 2007, pp. 2101-2107.

\bibitem{c8} K. He, G. Gkioxari, P. Dollar, and R. Girshick, "Mask R-CNN," in Proc. IEEE Int. Conf. Computer Vision, 2017, pp. 2961-2969.

\bibitem{c9} M. Quigley et al., "ROS: an open-source Robot Operating System," in ICRA Workshop on Open Source Software, 2009.

\bibitem{c10} G. Bradski, "The OpenCV Library," Dr. Dobb's Journal of Software Tools, 2000.

\end{thebibliography}




\end{document}
